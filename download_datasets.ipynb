{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import ast\n",
    "import random\n",
    "import PIL.Image as Image\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from aquabyte.lib.db.snowflake import snowflake_query_to_df\n",
    "SNOWFLAKE_DSN = '/dsn/snowflake/mochi'\n",
    "snowflake_query_to_df = partial(snowflake_query_to_df, ssm_name=SNOWFLAKE_DSN)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/workspace/mnt/\"\n",
    "TRAIN_SAVE_PATH = f\"{ROOT_DIR}data/train2017/\"\n",
    "VAL_SAVE_PATH = f\"{ROOT_DIR}data/val2017/\"\n",
    "ANNOTATION_SAVE_PATH = f\"{ROOT_DIR}data/annotations/\"\n",
    "VAL_FRAC = 0.1\n",
    "os.makedirs(TRAIN_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(VAL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(ANNOTATION_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# latest plali queues\n",
    "PLALI_QUEUE_NAMES = [\n",
    "    \"fish_detector_visibility_belsvik_and_ras_f3r\",\n",
    "    \"fish_bbox_fish_bbox_laksefjord_smolt_ras\",\n",
    "    \"fish_bbox_smolt_ras_gtsf_thumbnails_2024_01_19\",\n",
    "    #\"fish_bbox_imr_austevoll_jellyfish\",\n",
    "    \"fish_bbox_fish_bbox_100day_sampled\",\n",
    "    #\"fish_bbox_novasea_slaughter_line\",\n",
    "    #\"fish_bbox_synthetic_images\",\n",
    "    \n",
    "    #\"fish_bbox_toy_fish\",\n",
    "    #\"fish_bbox_penflix_plali_samples\",\n",
    "    #\"fish_bbox_in_air_gtsf\",\n",
    "]\n",
    "\n",
    "# these constants are used to load the dataset used to train the previous model\n",
    "OLD_BUCKET = \"s3://aquabyte-frames-resized-inbound/\"\n",
    "NEW_BUCKET = \"s3://aquabyte-research/pwais/mft-pg/datasets_s3/high_recall_fish1/images/\"\n",
    "S3_CSV_PATH = \"s3://aquabyte-research/pwais/mft-pg/datasets_s3/high_recall_fish1/hrf_with_keypoint_visibility_dataset.csv\"\n",
    "\n",
    "category_id_to_name_json = '{\"0\": \"HIGH\", \"1\": \"LOW\", \"2\": \"MEDIUM\", \"3\": \"PARTIAL\"}' \n",
    "id_to_category = json.loads(category_id_to_name_json)\n",
    "category_to_id = dict((c, i) for i, c in id_to_category.items())\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load annotations from PLALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the additional sql using PLALI_QUEUE_NAMES\n",
    "selection_criteria = \" or \".join([f\"startswith(pw.name, '{q}')\" for q in PLALI_QUEUE_NAMES])\n",
    "\n",
    "sql=f\"\"\"   \n",
    "select\n",
    "    --load json and select first element of pi.images\n",
    "    pi.images[0]::varchar as images,\n",
    "    pa.annotation:annotations as annotation,\n",
    "    -- find if the annotation is a skip by checking if skipReasons exist as a key\n",
    "    pa.annotation:skipReasons is not null as is_skip,\n",
    "    pa.plali_image_id,\n",
    "    --pa.annotator_email,\n",
    "    --pa.annotation_time,\n",
    "    pw.name\n",
    "from\n",
    "    prod.plali_workflows as pw\n",
    "    join prod.plali_images as pi on pi.workflow_id = pw.id\n",
    "    left join prod.plali_annotations as pa on pa.plali_image_id = pi.id\n",
    "where true\n",
    "    and is_skip = false\n",
    "    and ({selection_criteria})\n",
    "\"\"\"\n",
    "\n",
    "df_annotations = snowflake_query_to_df(sql)\n",
    "df_annotations['annotation'] = df_annotations['annotation'].apply(lambda x: json.loads(x) if x is not None else [])\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the original high recall fish detector training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m bucket, key \u001b[38;5;241m=\u001b[39m S3_CSV_PATH[\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m obj \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mget_object(Bucket\u001b[38;5;241m=\u001b[39mbucket, Key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[0;32m----> 4\u001b[0m df_hr_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(BytesIO(\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load the literals using ast for the following columns: images, metadata, label_set, original_annotation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df_hr_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_hr_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: ast\u001b[38;5;241m.\u001b[39mliteral_eval(x)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/response.py:99\u001b[0m, in \u001b[0;36mStreamingBody.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read at most amt bytes from the stream.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mIf the amt argument is omitted, read all data.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLLib3ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(endpoint_url\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39murl, error\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:482\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # use boto to load a csv file from s3\n",
    "bucket, key = S3_CSV_PATH[5:].split('/', 1)\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df_hr_dataset = pd.read_csv(BytesIO(obj['Body'].read()))\n",
    "# load the literals using ast for the following columns: images, metadata, label_set, original_annotation\n",
    "df_hr_dataset['images'] = df_hr_dataset['images'].apply(lambda x: ast.literal_eval(x)[0])\n",
    "df_hr_dataset['metadata'] = df_hr_dataset['metadata'].apply(lambda x: ast.literal_eval(x))\n",
    "df_hr_dataset['label_set'] = df_hr_dataset['label_set'].apply(lambda x: ast.literal_eval(x))\n",
    "df_hr_dataset['original_annotation'] = df_hr_dataset['original_annotation'].apply(lambda x: ast.literal_eval(x))\n",
    "df_hr_dataset['annotation'] = df_hr_dataset['annotation'].apply(lambda x: ast.literal_eval(x))\n",
    "df_hr_dataset['pen_id'] = df_hr_dataset['images'].apply(lambda x: x.split('/pen-id=')[1].split('/')[0])\n",
    "df_hr_dataset['captured_at'] = df_hr_dataset['images'].apply(lambda x: x.split('/at=')[1].split('/')[0])\n",
    "\n",
    "# replace the image path with the new bucket in images\n",
    "df_hr_dataset['images'] = df_hr_dataset['images'].apply(lambda x: x.replace(OLD_BUCKET, NEW_BUCKET))\n",
    "df_hr_dataset['is_skip'] = False\n",
    "\n",
    "#   drop columns starting with \"Unnamed\"\n",
    "df_hr_dataset = df_hr_dataset.loc[:, ~df_hr_dataset.columns.str.contains('^Unnamed')]\n",
    "df_hr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if original annotation is the same as annotation, which is indeed the case\n",
    "# df_hr_dataset['is_same'] = df_hr_dataset.apply(lambda x: x['original_annotation'].get('annotations', []) == x['annotation'], axis=1)\n",
    "\n",
    "# # get subset of the dataset where original_annotation is not the same as annotation\n",
    "# df_hr_dataset_not_same = df_hr_dataset[~df_hr_dataset['is_same']]\n",
    "# df_hr_dataset_not_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge datasets, download all images, create coco jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>annotation</th>\n",
       "      <th>is_skip</th>\n",
       "      <th>plali_image_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://aquabyte-research/pwais/mft-pg/datasets_s...</td>\n",
       "      <td>[{'label': 'PARTIAL', 'width': 207, 'xCrop': 3...</td>\n",
       "      <td>False</td>\n",
       "      <td>00000e98-e5cc-4788-a144-bd1fbd13822f</td>\n",
       "      <td>fish_detection_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://aquabyte-research/pwais/mft-pg/datasets_s...</td>\n",
       "      <td>[{'label': 'MEDIUM', 'width': 217, 'xCrop': 23...</td>\n",
       "      <td>False</td>\n",
       "      <td>0007dcbf-4210-4e75-b53f-a54dd892fb0d</td>\n",
       "      <td>fish_detection_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://aquabyte-research/pwais/mft-pg/datasets_s...</td>\n",
       "      <td>[{'label': 'HIGH', 'width': 244, 'xCrop': 203,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0010c967-c5b3-4e17-a1da-4a9e1c4b9f09</td>\n",
       "      <td>fish_detection_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://aquabyte-research/pwais/mft-pg/datasets_s...</td>\n",
       "      <td>[{'label': 'PARTIAL', 'width': 307, 'xCrop': 0...</td>\n",
       "      <td>False</td>\n",
       "      <td>00119e52-5dda-4b42-8dc2-4b760b4f1321</td>\n",
       "      <td>fish_detection_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://aquabyte-datasets-images/aquabyte-frames-...</td>\n",
       "      <td>[{'category': 'fish_visibility', 'height': 101...</td>\n",
       "      <td>False</td>\n",
       "      <td>0018c809-742e-4b31-8ade-718238561bd3</td>\n",
       "      <td>fish_detector_visibility_belsvik_and_ras_f3r_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15233</th>\n",
       "      <td>s3://aquabyte-datasets-images/aquabyte-frames-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>fish_detector_visibility_belsvik_and_ras_f3r_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15234</th>\n",
       "      <td>s3://aquabyte-datasets-images/aquabyte-frames-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>fish_detector_visibility_belsvik_and_ras_f3r_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15235</th>\n",
       "      <td>s3://aquabyte-datasets-images/aquabyte-frames-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>fish_detector_visibility_belsvik_and_ras_f3r_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15236</th>\n",
       "      <td>s3://aquabyte-datasets-images/aquabyte-frames-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>fish_detector_visibility_belsvik_and_ras_f3r_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15237</th>\n",
       "      <td>s3://aquabyte-datasets-images/aquabyte-frames-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>fish_detector_visibility_belsvik_and_ras_f3r_q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15238 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  images  \\\n",
       "0      s3://aquabyte-research/pwais/mft-pg/datasets_s...   \n",
       "1      s3://aquabyte-research/pwais/mft-pg/datasets_s...   \n",
       "2      s3://aquabyte-research/pwais/mft-pg/datasets_s...   \n",
       "3      s3://aquabyte-research/pwais/mft-pg/datasets_s...   \n",
       "4      s3://aquabyte-datasets-images/aquabyte-frames-...   \n",
       "...                                                  ...   \n",
       "15233  s3://aquabyte-datasets-images/aquabyte-frames-...   \n",
       "15234  s3://aquabyte-datasets-images/aquabyte-frames-...   \n",
       "15235  s3://aquabyte-datasets-images/aquabyte-frames-...   \n",
       "15236  s3://aquabyte-datasets-images/aquabyte-frames-...   \n",
       "15237  s3://aquabyte-datasets-images/aquabyte-frames-...   \n",
       "\n",
       "                                              annotation  is_skip  \\\n",
       "0      [{'label': 'PARTIAL', 'width': 207, 'xCrop': 3...    False   \n",
       "1      [{'label': 'MEDIUM', 'width': 217, 'xCrop': 23...    False   \n",
       "2      [{'label': 'HIGH', 'width': 244, 'xCrop': 203,...    False   \n",
       "3      [{'label': 'PARTIAL', 'width': 307, 'xCrop': 0...    False   \n",
       "4      [{'category': 'fish_visibility', 'height': 101...    False   \n",
       "...                                                  ...      ...   \n",
       "15233                                                 []    False   \n",
       "15234                                                 []    False   \n",
       "15235                                                 []    False   \n",
       "15236                                                 []    False   \n",
       "15237                                                 []    False   \n",
       "\n",
       "                             plali_image_id  \\\n",
       "0      00000e98-e5cc-4788-a144-bd1fbd13822f   \n",
       "1      0007dcbf-4210-4e75-b53f-a54dd892fb0d   \n",
       "2      0010c967-c5b3-4e17-a1da-4a9e1c4b9f09   \n",
       "3      00119e52-5dda-4b42-8dc2-4b760b4f1321   \n",
       "4      0018c809-742e-4b31-8ade-718238561bd3   \n",
       "...                                     ...   \n",
       "15233                                  None   \n",
       "15234                                  None   \n",
       "15235                                  None   \n",
       "15236                                  None   \n",
       "15237                                  None   \n",
       "\n",
       "                                                    name  \n",
       "0                                      fish_detection_v2  \n",
       "1                                      fish_detection_v2  \n",
       "2                                      fish_detection_v2  \n",
       "3                                      fish_detection_v2  \n",
       "4      fish_detector_visibility_belsvik_and_ras_f3r_q...  \n",
       "...                                                  ...  \n",
       "15233  fish_detector_visibility_belsvik_and_ras_f3r_q...  \n",
       "15234  fish_detector_visibility_belsvik_and_ras_f3r_q...  \n",
       "15235  fish_detector_visibility_belsvik_and_ras_f3r_q...  \n",
       "15236  fish_detector_visibility_belsvik_and_ras_f3r_q...  \n",
       "15237  fish_detector_visibility_belsvik_and_ras_f3r_q...  \n",
       "\n",
       "[15238 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat the two dataframes, keeping only the columns that are in df_annotations\n",
    "df_hr_dataset = df_hr_dataset[df_annotations.columns]\n",
    "df_hr_dataset = pd.concat([df_hr_dataset, df_annotations], ignore_index=True)\n",
    "# sort by plali_image_id\n",
    "df_hr_dataset = df_hr_dataset.sort_values(by='plali_image_id').reset_index(drop=True)\n",
    "df_hr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████▉                          | 2421/15238 [27:40<1:32:53,  2.30it/s]"
     ]
    }
   ],
   "source": [
    "# download the images from images column to the local machine, save them in the IMAGE_SAVE_PATH\n",
    "local_paths = []\n",
    "not_found = []\n",
    "is_val = []\n",
    "rand = random.Random(1337)\n",
    "\n",
    "for i, row in tqdm(df_hr_dataset.iterrows(), total=df_hr_dataset.shape[0]):\n",
    "    is_val_image = rand.random() < VAL_FRAC\n",
    "    remote_path = row['images']\n",
    "    file_extension = remote_path.split('.')[-1]\n",
    "    if is_val_image:\n",
    "        local_path = f\"{VAL_SAVE_PATH}{row.plali_image_id}.{file_extension}\"\n",
    "        is_val.append(True)\n",
    "    else:\n",
    "        local_path = f\"{TRAIN_SAVE_PATH}{row.plali_image_id}.{file_extension}\"\n",
    "        is_val.append(False)\n",
    "    bucket = remote_path.split('/')[2]\n",
    "    key = '/'.join(remote_path.split('/')[3:])\n",
    "    local_paths.append(local_path)\n",
    "    # check if the file already exists\n",
    "    if os.path.exists(local_path):\n",
    "        continue\n",
    "    # download the file, catch if the file is not found\n",
    "    try:\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "    except Exception as e:\n",
    "        not_found.append(row.plali_image_id)\n",
    "        print(f\"{remote_path}\\n{e}\")\n",
    "        print(f\"{bucket}/{key}\")\n",
    "df_hr_dataset['local_path'] = local_paths\n",
    "df_hr_dataset['is_val_image'] = is_val\n",
    "# drop the rows where the image was not found\n",
    "df_hr_dataset_found = df_hr_dataset[~df_hr_dataset['plali_image_id'].isin(not_found)]\n",
    "df_hr_dataset_found = df_hr_dataset_found.reset_index(drop=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the images and get the image height and width\n",
    "image_heights = []\n",
    "image_widths = []\n",
    "for i, row in tqdm(df_hr_dataset_found.iterrows(), total=df_hr_dataset_found.shape[0]):\n",
    "    local_path = row['local_path']\n",
    "    im = Image.open(local_path)\n",
    "    image_heights.append(im.height)\n",
    "    image_widths.append(im.width)\n",
    "df_hr_dataset_found['image_height'] = image_heights\n",
    "df_hr_dataset_found['image_width'] = image_widths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through df_hr_dataset_found.annotations and check each annotation has a label, width, height, xCrop, and yCrop, if there is a missing value, drop the dict\n",
    "def clean_annotations(annotations:list):\n",
    "    for annotation in annotations:\n",
    "        if any([annotation.get('label') is None, annotation.get('width') is None, annotation.get('height') is None, annotation.get('xCrop') is None, annotation.get('yCrop') is None]):\n",
    "            print(f\"dropping annotation {annotation} from {annotations} because it is missing a value\")\n",
    "            time.sleep(1)\n",
    "            annotations.remove(annotation)\n",
    "    return annotations\n",
    "\n",
    "df_hr_dataset_found['annotation'] = df_hr_dataset_found['annotation'].apply(clean_annotations)\n",
    "#save the dataframe to a csv file\n",
    "df_hr_dataset_found.to_csv(f\"{ROOT_DIR}hrf_with_keypoint_visibility_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/aquabyte-new/research-exploration/blob/master/pwais/mft-pg/mft_utils/coco_dataset.py\n",
    "annos_val = []\n",
    "images_val = []\n",
    "annotations = []\n",
    "images = []\n",
    "val_images = []\n",
    "\n",
    "for _, row in df_hr_dataset_found.iterrows():\n",
    "    is_val_image = row.is_val_image\n",
    "    img_idx = len(val_images) if is_val_image else len(images)\n",
    "    for bbox in row.annotation:\n",
    "        anno_id = len(annos_val) + 1 if is_val_image else len(annotations) + 1\n",
    "        category_id = category_to_id[bbox['label']]\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h = bbox['xCrop'], bbox['yCrop'], bbox['width'], bbox['height']\n",
    "        anno = {\n",
    "            \"id\": anno_id,\n",
    "            \"image_id\": img_idx,\n",
    "            \"category_id\": category_id,\n",
    "            \"bbox\": [bbox_x, bbox_y, bbox_w, bbox_h],\n",
    "            # \"keypoints\": [],\n",
    "            # \"num_keypoints\": 0,\n",
    "            \"score\": -1,\n",
    "            \"area\": bbox_w * bbox_h,\n",
    "            \"iscrowd\": 0,\n",
    "        }\n",
    "        if is_val_image:\n",
    "            annos_val.append(anno)\n",
    "        else:\n",
    "            annotations.append(anno)\n",
    "\n",
    "        img_path = row.local_path\n",
    "        img_fname = os.path.basename(img_path)\n",
    "\n",
    "        image = {\n",
    "            \"id\": img_idx,\n",
    "            \"file_name\": img_fname,\n",
    "            \"height\": row.image_height,\n",
    "            \"width\": row.image_width,\n",
    "            \"source_name\": row.name,\n",
    "        }\n",
    "        if is_val_image:\n",
    "            val_images.append(image)\n",
    "        else:\n",
    "            images.append(image)\n",
    "            \n",
    "categories = [\n",
    "{'id': id_, 'name': name}\n",
    "for name, id_ in category_to_id.items()\n",
    "]\n",
    "\n",
    "train_json = {\n",
    "'categories': categories,\n",
    "'images': images,\n",
    "'annotations': annotations,\n",
    "}\n",
    "\n",
    "val_json = {\n",
    "'categories': categories,\n",
    "'images': val_images,\n",
    "'annotations': annos_val,\n",
    "}\n",
    "\n",
    "with open(f\"{ANNOTATION_SAVE_PATH}train.json\", \"w\") as f:\n",
    "    json.dump(train_json, f)\n",
    "\n",
    "with open(f\"{ANNOTATION_SAVE_PATH}val.json\", \"w\") as f:\n",
    "    json.dump(val_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
